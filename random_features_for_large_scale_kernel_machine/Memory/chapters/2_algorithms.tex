\chapter{Algorithms}
% Derivación de los algoritmos de aprendizaje propuestos + pseudocódigo de dichos algoritmos.


We are going to explore two sets of random features. 

\section{Random Fourier Features}

Let be $x \in \R^d$  (a column vector), the first set of random features consists of random 
Fourier bases
\begin{equation}
    \cos(w^T x + b) 
\end{equation}
where $w \in \R^d$  and $b$ are random variables. 

See that $T(x) = w^t x +b$ is affine transformation
$T:\R^d \longrightarrow \R$  and then $\cos$ function maps
$cos: \R  \longrightarrow S^1$. 

The algorithm is defined as: 

\begin{algorithm}[hbt!]
    \caption{Random Fourier Features}\label{alg:two}
    \KwData{$K$ a positive definite shift-invariant kernel $k(x,y) = k(x - y).$}
    \KwResult{A randomized feature map $z(x): \R^d \longrightarrow \R^D$
     so that $z(x)^T z(y) \approx k(x - y)$}

     Compute the Fourier transform $p$ of the kernel $k$: 
    \begin{equation}\label{eq:fourier_transformation}
        p(w) = \frac{1}{2 \pi}
        \int
        e^{-jw^T \delta}k(\delta) 
        d \Delta. 
     \end{equation}
        \\
     Draw $D$ iid samples 
     $\{w_1, \ldots, w_D \} \subset \R^d$ from $p$ and
     $D$ iid samples $b_1, \ldots, b_D \in \R$
     from the uniform distribution on 
     $[0, 2\pi]$. 
     \\
     Let 
     \begin{equation}
        z(x)
        \equiv
        \sqrt{\frac{2}{D}}
        \left[ 
            \cos(w_1^T x + b_1) 
            \, 
            \ldots
            \cos(w_D^T x + b_D) 
            \right]^T. 
     \end{equation}
    \end{algorithm}


\subsection{Implementation considerations}
\subsubsection{Fourier transformation}

In order to compute (\ref{eq:fourier_transformation})
we have different alternatives: 

\begin{itemize}
    \item Use (\ref{eq:fourier_transformation}) and a numerical method to compute the integral (as Montecarlo). 
    \item Use a library \textit{scipy Fourier Transforms}\footnote{
        See \url{https://docs.scipy.org/doc/scipy/tutorial/fft.html} (last consult on 03-07-23).
    }
    \item Write explicitly its formula. 
\end{itemize}

For simplicity and since we use widely known kernels we are going to follow the third alternative. 
The implementation of the kernel and its Fourier Transformation can be found at \texttt{/Code/random\_feature/kernels.py}. 

\subsubsection{How to draw example from a probability distribution $p$}

Let $p : \R^d \longrightarrow \R$ a probability function. In order to draw samples from this distribution 
we are going to use the following algorithm: 



\begin{algorithm}[hbt!]
    \caption{Generate example from a probability distribution }\label{alg:two}
    \KwData{
        \begin{itemize}
            \item Probability density function $p : \R^d \longrightarrow \R$,
            \item dimension of the output vectors $d$, 
            \item number of vectors: $n$,
            \item maximum absolute value of the coefficients of the vectors: $K$. 
        \end{itemize}
    }
    \KwResult{
        Radom variables: 
        $\left\{w_1, \ldots, w_n\right\} \subset \R^d$ r
        that are examples of $p$. 
        }
  
        Let $\Lambda \gets \{\}$ be the solution. 
        \\
        \While{The size of $\Lambda$ is less than $n$}{
            Let $w \gets U([-k, k]^d)$ \Comment*[r]{random vector of $d$ vector that follow a uniform distribution in $[-k, k]$ interval}
            %
            $$u \gets U([0,1])$$
            %
            \If{$u \leq p(w)$}{
               $$
               \Lambda \gets  \Lambda \cup \{w\}.
               $$ 
            }
        }
        \Return{$\Lambda$} 
    \end{algorithm}

Ideally, $K$ should be as big as  possible. 
However, the bigger $K$ is, the bigger the norm of the vectors $w$
and the lower their probability. 

If the probability of acceptance is too low, we would end up rejecting too many vectors, making the algorithm slow. Therefore, so we need to find a balance 
between mathematical rigor and pragmatism choosing $k$. 

This function is done at \texttt{random\_feature/random\_samples.py}. 


\subsection{Some popular shift-invariant kernels and their Fourier transforms}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
     Kernel name & $k(x-y)$ & p(w) \\
     \hline
     % Gaussian kernel 
     Gaussian 
     & $e^{- \frac{\| x- y\|^2_2}{2}}$
    & 
    $(2 \pi)^{-\frac{D}{2}} e^{- \frac{\| w\|^2_2}{2}}$ 
    \\
    \hline
    \end{tabular}
    \caption{Some popular shift-invariant kernels and their Fourier transforms}
\end{table}


\subsection{Convergence proof}

Our objective is to proof that $z(x)^Tz(y)$ is close to $k(x-y)$. 
Mathematically, we want to garante the uniform convergence of the Fourier Features. 

% TODO add scheme of the proof

\begin{theorem}[Bochner's theorem]
    A continuos kernel $k(x,y) = k(x-y)$ on $\R^d$ is positive 
    if and only if $k(\delta)$ is the Fourier transforme of a non-negative measure. 
\end{theorem}

If a shift-invariant kernel $k(\delta)$ is properly scaled, Bochner’s theorem guarantees that its Fourier transform 
$p(w)$
is a proper probability distribution.

Defining $\zeta_w(x) = e^{jw^tx}$, we have

\begin{equation}
    \label{eq:bochnerTheorem}
    k(x-y)
    =
    \int_{\R^d}
    p(w)
    e^{jw^t(x-y)}
    dw
    = 
    E_w\left[
        \zeta_w(x)
        \zeta_w(y)^*
    \right],
\end{equation}
where $\zeta_w(y)^* =  e^{- jw^tx}$ is the conjugate. 
We have proof in (\ref{eq:bochnerTheorem}) that $  \zeta_w(x)
\zeta_w(y)^*$ is a unbiased estimate of $k(x,y)$ when $w$ is drawn from $p$. 

Notice that $p: \R^d \longrightarrow [0,1]$ is a real function, 
and the $k(\Delta)$ is real too, so the integral (\ref{eq:bochnerTheorem})
converges when $\zeta_w(x)
\zeta_w(y)^*$ is real. By Euler's formula: 
\begin{equation}
    \zeta_w(\Delta) 
    = 
    e^{jw^T \Delta} 
    = \cos \left(
        -w^t\Delta
    \right)
    +
    j \sin \left(
        -w^t\Delta
    \right)
    = \cos \left(
        w^t\Delta
    \right).
\end{equation}


