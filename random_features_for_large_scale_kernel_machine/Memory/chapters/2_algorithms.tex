\chapter{Algorithms}
% Derivación de los algoritmos de aprendizaje propuestos + pseudocódigo de dichos algoritmos.


We are going to explore two sets of random features. 

\section{Random Fourier Features}

Let be $x \in \R^d$  (a column vector), the first set of random features consists of random 
Fourier bases
\begin{equation}
    \cos(w^T x + b) 
\end{equation}
where $w \in \R^d$  and $b$ are random variables. 

See that $T(x) = w^t x +b$ is affine transformation
$T:\R^d \longrightarrow \R$  and then $\cos$ function maps
$\cos: \R  \longrightarrow S^1$. 

The algorithm is defined as: 

\begin{algorithm}[hbt!]
    \caption{Random Fourier Features}\label{alg:two}
    \KwData{$K$ a positive definite shift-invariant kernel $k(x,y) = k(x - y).$}
    \KwResult{A randomized feature map $z(x): \R^d \longrightarrow \R^D$
     so that $z(x)^T z(y) \approx k(x - y)$}

     Compute the Fourier transform $p$ of the kernel $k$: 
    \begin{equation}\label{eq:fourier_transformation}
        p(w) = \frac{1}{2 \pi}
        \int
        e^{-jw^T \delta}k(\delta) 
        d \Delta. 
     \end{equation}
        \\
     Draw $D$ iid samples 
     $\{w_1, \ldots, w_D \} \subset \R^d$ from $p$ and
     $D$ iid samples $b_1, \ldots, b_D \in \R$
     from the uniform distribution on 
     $[0, 2\pi]$. 
     \\
     Let 
     \begin{equation}
        z(x)
        \equiv
        \sqrt{\frac{2}{D}}
        \left[ 
            \cos(w_1^T x + b_1) 
            \, 
            \ldots
            \cos(w_D^T x + b_D) 
            \right]^T. 
     \end{equation}
    \end{algorithm}


\subsection{Implementation considerations}
\subsubsection{Fourier transformation}

In order to compute (\ref{eq:fourier_transformation})
we have different alternatives: 

\begin{itemize}
    \item Use (\ref{eq:fourier_transformation}) and a numerical method to compute the integral (as Montecarlo). 
    \item Use a library \textit{scipy Fourier Transforms}\footnote{
        See \url{https://docs.scipy.org/doc/scipy/tutorial/fft.html} (last consult on 03-07-23).
    }
    \item Write explicitly its formula. 
\end{itemize}

For simplicity and since we use widely known kernels we are going to follow the third alternative. 
The implementation of the kernel and its Fourier Transformation can be found at \texttt{/Code/random\_feature/kernels.py}. 

\subsubsection{How to draw examples from a probability distribution $p$}

Let $p : \R^d \longrightarrow \R$ a probability function. In order to draw samples from this distribution 
we are going to use the following algorithm: 



\begin{algorithm}[hbt!]
    \caption{Generate example from a probability distribution }\label{alg:two}
    \KwData{
        \begin{itemize}
            \item Probability density function $p : \R^d \longrightarrow \R$,
            \item dimension of the output vectors $d$, 
            \item number of vectors: $n$,
            \item maximum absolute value of the coefficients of the vectors: $K$. 
        \end{itemize}
    }
    \KwResult{
        Radom variables: 
        $\left\{w_1, \ldots, w_n\right\} \subset \R^d$ r
        that are examples of $p$. 
        }
  
        Let $\Lambda \gets \{\}$ be the solution. 
        \\
        \While{The size of $\Lambda$ is less than $n$}{
            Let $w \gets U([-k, k]^d)$ \Comment*[r]{random vector of $d$ vector that follow a uniform distribution in $[-k, k]$ interval}
            %
            $$u \gets U([0,1])$$
            %
            \If{$u \leq p(w)$}{
               $$
               \Lambda \gets  \Lambda \cup \{w\}.
               $$ 
            }
        }
        \Return{$\Lambda$} 
    \end{algorithm}

Ideally, $K$ should be as big as  possible. 
However, the bigger $K$ is, the bigger the norm of the vectors $w$
and the lower their probability. 

If the probability of acceptance is too low, we would end up rejecting too many vectors, making the algorithm slow. Therefore, so we need to find a balance 
between mathematical rigor and pragmatism choosing $k$. 

This function is done at \texttt{random\_feature/random\_samples.py}. 


\subsection{Some popular shift-invariant kernels and their Fourier transforms}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
     Kernel name & $k(x-y)$ & p(w) \\
     \hline
     % Gaussian kernel 
     Gaussian 
     & $e^{- \frac{\| x- y\|^2_2}{2}}$
    & 
    $(2 \pi)^{-\frac{D}{2}} e^{- \frac{\| w\|^2_2}{2}}$ 
    \\
    \hline
        % Laplacian 
        $\R^2$ Laplacian 
        & 
        $e^{
            - \|\Delta\|_1
        }$
        &
        $\prod_{d = 1}^D \frac{1}{\pi(1+w_d^2)}$
        \\
    \hline
        % Cauchy 
        Cauchy 
        &
        $\prod_{d = 1}^D \frac{2}{o}$
        &
        aquí no sé qué va
    \\
    \hline
    \end{tabular}
    \caption{Some popular shift-invariant kernels and their Fourier transforms}
\end{table}


\subsection{Convergence proof}

Our objective is to proof that $z(x)^Tz(y)$ is close to $k(x-y)$. 
Mathematically, we want to garante the uniform convergence of the Fourier Features. 

% TODO add scheme of the proof

\begin{theorem}[Bochner's theorem]
    A continuos kernel $k(x,y) = k(x-y)$ on $\R^d$ is positive 
    if and only if $k(\delta)$ is the Fourier transforme of a non-negative measure. 
\end{theorem}

If a shift-invariant kernel $k(\delta)$ is properly scaled, Bochner’s theorem guarantees that its Fourier transform 
$p(w)$
is a proper probability distribution.

Defining $\zeta_w(x) = e^{jw^tx}$, we have

\begin{equation}
    \label{eq:bochnerTheorem}
    k(x-y)
    =
    \int_{\R^d}
    p(w)
    e^{jw^t(x-y)}
    dw
    = 
    E_w\left[
        \zeta_w(x)
        \zeta_w(y)^*
    \right],
\end{equation}
where $\zeta_w(y)^* =  e^{- jw^tx}$ is the conjugate. 
We have proof in (\ref{eq:bochnerTheorem}) that $  \zeta_w(x)
\zeta_w(y)^*$ is a unbiased estimate of $k(x,y)$ when $w$ is drawn from $p$. 

Notice that $p: \R^d \longrightarrow [0,1]$ is a real function, 
and the $k(\Delta)$ is real too, so the integral (\ref{eq:bochnerTheorem})
converges when $\zeta_w(x)
\zeta_w(y)^*$ is real.
% By Euler's formula: 
% \begin{equation}
%     \zeta_w(\Delta) 
%     = 
%     e^{jw^T \Delta} 
%     = \cos \left(
%         -w^t\Delta
%     \right)
%     +
%     j \sin \left(
%         -w^t\Delta
%     \right)
%     = \cos \left(
%         w^t\Delta
%     \right).
% \end{equation}

Defining 
\begin{equation}
    z_w(x) = \sqrt{2} \cos\left(w^T x + b\right), 
\end{equation}
where $w$ is drawn from a $p(w)$ and $b$ is drawn 
uniformly from $[0,2\pi]$ we obtain that 

\begin{equation} \label{unbiases_stimator}
    E \left[z_w(x) z_w(y) \right] = k(x,y). 
\end{equation}

\begin{proof}{
Firstly, for $s \in \{1,-1\}$ notice that using chain rule and 
$p(w)$ is a probability function and therefore 
$\int_{\R^d} p(w) dw = 1$.
\begin{align}
    \label{eq:expected_value}
    E\left[ e^{sj w^T(x+y)+s2b}\right]
    & = 
    \int_{\R^d} e^{sj w^T(x+y)+s2b} p(w) dw
    \nonumber
    \\
    \nonumber
    & = 
    e^{s2b}\int_{\R^d} e^{sj w^T(x+y)} p(w) dw
    \\
    \nonumber
    & = 
    e^{s2b}\left\{ 
        e^{sj w^T(x+y)} 
        - 
        \int j(x+y) e^{sj w (x + y)} dw
    \right\}_{\R^d}
    \\
    & = 
    e^{s2b}\left\{ 
        e^{sj w^T(x+y)} 
        - 
         e^{sj w (x + y)} 
    \right\}
    = 0.
\end{align}

Secondly, as a consequence of the sum of angles: 

\begin{align}
    z_w(x) z_w(y) 
    &=
    2  \cos\left(w^T x + b\right) \cos\left(w^T y+ b\right)
    \nonumber
    \\
    \nonumber
    & = 
    \left(
    \cos\left(w^T x + b\right) \cos\left(w^T y+ b\right)
    + 
    \sin\left(w^T x + b\right) \sin\left(w^T y+ b\right)
    \right)
    \\
    \nonumber
   & \quad +
   \left(
    \cos\left(w^T x + b\right) \cos\left(w^T y+ b\right)
    -
    \sin\left(w^T x + b\right) \sin\left(w^T y+ b\right)
    \right)
    \\
    & = 
    \cos\left(w^T (x-y) \right) + \cos\left(w^T (x+y) + 2b \right).
\end{align}

Thanks to Euler formula
\begin{equation}
    \cos\left(w^T (x-y) \right)
    = 
    \frac{1}{2}
    \left(
        e^{j w^T (x-y)}
        + 
        e^{- j w^T (x-y)}
    \right)
    = 
     \frac{1}{2}
    \left(
        e^{j w^T (x-y)}
        + 
        e^{j w^T (y-x)}
    \right),
\end{equation}
and its expected value is 
\begin{align}
    E\left[
        \cos\left(w^T (x-y) \right)
    \right]
    = 
    \frac{1}{2}
    \left(
        E\left[
         e^{j w^T (x-y)}
        \right]
        + 
        E\left[
         e^{j w^T (y-x)}
        \right]
    \right)
    = k(x,y),
\end{align}
since $k$ is symmetric and shift invariant and (\ref{eq:bochnerTheorem}).

Finally, as a result of Euler formula and (\ref{eq:expected_value})
\begin{equation}
    E\left[
        \cos\left(w^T (x+y) + 2b \right)
    \right]
    = 
    0, 
\end{equation}
so we have proved (\ref{unbiases_stimator}).
} % end of the proof



\begin{theorem}{Hoeffding's inequality}
    Let $X_1, \ldots, X_n$ be independent random variables
    such that $a_i \leq X_i \leq b_i$ almost surely.
    Consider $S_n = X_1 +  \ldots + X_n$.

    The Hoeffding's theorem states that, for all $t>0$,
    \begin{equation}
        P\left(
            |S_n - E[S_n]| \geq t
        \right)
        \leq 
        2 \exp \left(
            - \frac{2 t^2}{\sum_{i=1}^n (b_i -a_i)^2}
        \right)
    \end{equation}
\end{theorem}
(See the proof at \cite{Hoeffding1994}). 


%% Por aquí empieza la Claim que no sé qué hace 

\begin{claim}
    (Uniform convergence of Fourier features) 
    Let $M$ be a compact subset of $\mathbb{R}^d$ with diameter $\operatorname{diam}(M)$. Then, for the mapping $z$ defined in Algorithm 1, we have
    \begin{align}
        P \left[
            \sup_{x,y \in M} |z(x)^T z(y) - k(y,x)|
            \geq \varepsilon
        \right]
        \leq 
        2^8
        \left(
            \frac{\sigma_p \operatorname{diam}(M)}{\varepsilon}
        \right)
        \exp \left(
            - \frac{D \varepsilon^2}{4(d+2)},
        \right)
    \end{align}
    where $\sigma_p^2 \equiv \mathbb{E}_{p(\omega)}[\omega' \omega]$ is the second moment of the Fourier transform of $k$.

    Further, 
    $$
    \sup_{x,y \in M} |z(x)^T z(y) - k(y,x)|
    \geq \varepsilon
    $$
    with any constant probability when $D = \Omega \left( a \right)$
\end{claim}

\begin{proof}
    

\end{proof}


\section{Random Binning Features}  

The second randomized mapping partitions the input space by utilizing grids that are randomly shifted and have random resolutions. It assigns a binary bit string to each input point, based on the bin in which the point falls. Since this mapping uses rectilinear grids, it is particularly suitable for kernels that depend solely on $L_1$.

The grids are designed such that the likelihood of two points $x$ and $y$ being assigned to the same bin is proportional to $k(x,y)$. The inner product between two transformed points is proportional to the number of times they are binned together, providing an unbiased estimate of $k(x,y)$.

% Corregido hasta aquí

Now we are going to define the kernel. Firstly we define a real (one dimension) approach: 

Let  $k_{hat}(x, y; \delta) : \R \times \R \times ^+$ be the kernel defined as: 
\begin{equation}
    k_{hat}(x, y; \delta)
    = 
    \max 
    \left(
    0,
    1 - \frac{|x-y|}{\delta}
    \right)  
\end{equation}

Notice that 

\begin{equation}
    1 - \frac{|x-y|}{\delta} > 0
    \Leftrightarrow
    \delta > |x-y |
\end{equation}
and since $\frac{|x-y|}{\delta} > 0$ for every $x,y \in \R$
\begin{equation}
     0 \leq k_{hat}(x, y; \delta) \leq 1.
\end{equation} 
Moreover, this induce a real partition of pitch $\delta$ and $k_{hat}$ is the probability that two pints $x,y$ falls in the same grid since 
they would be in the same grid if 

\begin{equation}
 \hat{x} = \hat{y}
   \Leftrightarrow
        \left\lfloor
            \frac{x-u}{\delta}
        \right\rfloor
        = 
        \left\lfloor
        \frac{y-u}{\delta}
    \right\rfloor
.
\end{equation}
It would happen if
\begin{equation}
    \left| 
        \hat{x} - \frac{x-u}{\delta}
    \right|
    \leq 0.5
    \text{ and }
    \left| 
        \hat{x} - \frac{y-u}{\delta}
    \right|
    \leq 0.5.
\end{equation}
Adding both constraint and using triangle inequality we obtain 
\begin{equation}
    1 \geq \left| 
        \frac{x-u}{\delta} - \frac{y-u}{\delta}
    \right|
    \geq
        \frac{ |x-y|}{\delta}.
\end{equation}


\textbf{TODO falta escribir esto mejor} 

So fixed $\delta$ the probability of two points $x,y$  to fall in the same bin is the grid $k_{hat}(x,y,\delta)$. 

If we encode $\hat{x}$ as a binary indicator vector $z(x)$ over the bins, 
$z(x)^T z(y) = 1$ if $x$ and $y$ fall in the same bin and zero otherwise, so 
\begin{equation}
    P\left[
        z(x)^T z(y) = 1 | \delta
    \right]
    = 
    E\left[
        z(x)^T z(y) = 1 | \delta
    \right]
    = 
    k_{hat}(x,y,\delta). 
\end{equation}
therefore $z$ is a random map for $k_{hat}$. 
%%
Let us now consider shift-invariant kernels that can be expressed as convex combinations of hat kernels on a compact subset of $\R \times \R$. Mathematically, this can be written as:

\begin{equation}
k(x,y) = \int_{0}^\infty k_{hat}(x,y,\delta) p(\delta) d\delta,
\end{equation}

where $k_{hat}(x,y,\delta)$ is the hat kernel with pitch $\delta$, $p(\delta)$ is a probability distribution over pitch values, and the integral is taken over all positive pitch values. In this formulation, if the pitch $\delta$ of the grid is sampled from $p$, and the shift $u$ is drawn uniformly from $[0,\delta]$, then the probability that $x$ and $y$ are binned together is given by $k(x,y)$.

Before to continue we need to proof the following result: 
\begin{lemma}
    \label{eq:lema1}
    Suppose a function $k(\Delta): \mathbb{R} \rightarrow \mathbb{R}$ is twice differentiable and has the form
    \begin{equation}
        k(\Delta) = \int_{\mathbb{R}} p(\delta) \max(0, 1-\Delta \delta) d\delta.
    \end{equation}
    Then $p(\delta) = \delta k''(\delta)$.
\end{lemma}
\begin{proof}
    \begin{align}
        k(\Delta) &= 
        \int_{\mathbb{R}} p(\delta) \max(0, 1-\Delta \delta) d\delta
        \\ 
        & = 
        % cero
        \int_{0}^\Delta p(\delta) 0 d\delta
        + 
        \int_{\Delta}^\infty
            p(\delta)
            \left(1 - \frac{\Delta}{\delta}\right) 
            d \delta
        \\
        &=
        \int_{\Delta}^\infty
            p(\delta)
            d \delta
        -
        \int_{\Delta}^\infty
            \frac{p(\delta)}{\delta}
            d \delta. 
    \end{align}

Applying the Fundamental theorem of calculus and chain rules: 
\begin{equation}
   k'(\Delta)
   = 
   - p(\Delta)
   - 
   \left[
    \int_{\Delta}^\infty
            \frac{p(\delta)}{\delta}
            d \delta
    -
    \Delta \frac{p(\Delta)}{\Delta}
   \right] 
   = - \int_{\Delta}^\infty
   \frac{p(\delta)}{\delta}
   d \delta.
\end{equation}
Applying again the Fundamental theorem oc calculus: 

\begin{equation}
    k''(\Delta)
    = 
    \frac{P(\Delta)}{\Delta}. 
\end{equation}
\end{proof}

% Continuamos con el resultado  

Lema 1 \ref{eq:lema1} shows that the function $p$ can be easily recovered from $k$ by setting 
\begin{equation}
    p(\delta) = \delta k''(\delta).   
\end{equation}

 For example, when considering the Laplacian kernel with $k_{\text{Laplacian}}(x, y) = \exp(-|x - y|)$, we have that $p(\delta)$ is the Gamma distribution given by $\delta \exp(-\delta)$. However, for the Gaussian kernel, $k(\delta)$ is not convex, resulting in $k''$ not being everywhere positive, and thus $\delta k''(\delta)$ is not a probability distribution. Therefore, this procedure does not yield a random map for the Gaussian.


 Random maps for separable multivariate shift-invariant kernels of the form 
 \begin{equation}
    k(x - y) = \prod_{m=1}^d k_m(|x_m - y_m|)
 \end{equation}
  (such as the multivariate Laplacian kernel) 
  can be constructed similarly if each $k_m$ can be expressed as a convex combination of hat kernels. 
  
  The above binning process is applied independently to each dimension of $\mathbb{R}^d$. The probability that $x^m$ and $y^m$ are binned together in dimension $m$ is $k_m(|x^m - y^m|)$. 
  
  Since the binning process is independent across dimensions, the probability that $x$ and $y$ are binned together in every dimension is 
  \begin{equation}
    \prod_{m=1}^d k_m(|x_m - y_m|) = k(x - y).
  \end{equation}
   
  In this multivariate case, $z(x)$ encodes the integer vector $[\hat{x}^1, \dots, \hat{x}^d]$ corresponding to each bin of the $d$-dimensional grid as a binary indicator vector. In practice, to prevent overflows when computing $z(x)$ for large $d$, unoccupied bins are eliminated from the representation. Since there are never more bins than training points, this ensures that no overflow can occur.

  We can again reduce the variance of the estimator $z(x)^Tz(y)$ by concatenating $P$ random binning functions $z$ into a larger list of features $z$ and scaling by $\sqrt{\frac{1}{P}}$. The inner product 
  \begin{equation}
    z(x)'z(y) = \frac{1}{P}\sum_{p=1}^P z_p(x)'z_p(y)
  \end{equation}
 is the average of $P$ independent $z(x)^Tz(y)$ and therefore has lower variance.

%%% 
TODO improve: 


\begin{algorithm}[hbt!]
    \caption{Random Binning Features}\label{alg:RandomBinning Features}
    \KwData{
        \begin{itemize}
            \item A point $x \in \R^d$.
            \item A kernel function $k(x,y) =  \prod_{m=1}^d k_m(|x^m - y^m|)$, so that $p_m(\Delta) \equiv \Delta k_m''(\Delta)$ is a probability distribution on $\Delta \geq 0$.
            \item a randomized feature map $z(x)$ so that $z(x)^Tz(y) \approx k(x-y)$.
        \end{itemize}
    }
    \KwResult{
        $$
        z(x) 
        \equiv 
        \sqrt{\frac{1}{P}} 
        \left[z_1(x), \ldots, z_p(x)\right]^T.
        $$
    }
        \For{$p \in \{1, \ldots, P\}$ }{
            Draw grid parameters $\delta, u \in \mathbb{R}^d$ with the pitch $\delta^m \sim p_m$, and shift $u^m$ from the uniform distribution on $[0, \delta^m]$.
            \\
             Let $z$ return the coordinate of the bin containing $x$ as a binary indicator vector 
             $$
             z_p(x) \equiv 
             \text{hash}
             \left(
                \left\lfloor 
                    \frac{x^1 - u^1}{\delta^1} 
                \right\rfloor,
                 \dots, 
                 \left\lfloor
                    \frac{x^d - u^d}{\delta^d} 
                \right\rfloor
             \right).
             $$ 
        }
    \end{algorithm}




% Esto es del claim 1 que se me ha colado 
\begin{lemma}
    Define $s(x,y) \equiv z(x)^Tz(y)$, and $f(x,y) \equiv s(x,y) - k(y,x)$, and recall that $|f(x,y)| \leq 2$ and $\mathbb{E}[f(x,y)] = 0$ by construction. Since $f$ and $s$ are shift invariant, as their arguments we use $\Delta \equiv x - y \in M_\Delta$ for notational simplicity. 
\end{lemma}
\begin{proof}
    $M_\Delta$ is compact and has diameter at most twice $\operatorname{diam}(M)$, so we can find an $\varepsilon$-net that covers $M^\Delta$ using at most $T = \left(\frac{4\operatorname{diam}(M)}{r}\right)^d$ balls of radius $r$ \cite{15}. Let ${\Delta_i}_{i=1}^T$ denote the centers of these balls, and let $L_f$ denote the Lipschitz constant of $f$. We have $|f(\Delta)| < \varepsilon$ for all $\Delta \in M^\Delta$ if $|f(\Delta_i)| < \frac{\varepsilon}{2}$ and $L_f < \varepsilon$ for all $i$. We bound the probability of these two events.
\end{proof}