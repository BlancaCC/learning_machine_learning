\section{ Fast and scalable polynomial kernels via explicit feature maps}
\begin{frame}
    \frametitle{Fast and scalable polynomial kernels via explicit feature maps}
\begin{itemize}
    \item \cite{fast-and-scalable-polynomial-kernels}
    \item Cited by 296. 
    \item Year 2013. 
\end{itemize}

\textbf{Util for us}
\begin{itemize}
    \item Good approach for polynomial kernel 
    \begin{equation}
        k(x,y)
        = 
        (\langle x, y  \rangle + c)^p. 
    \end{equation}
\end{itemize}


\end{frame}


\begin{frame}
    \frametitle{Content}
    \begin{itemize}
        \item They introduce a fast and scalable randomized tensor product technique for approximating polynomial kernels,
        
        \item Accelerating the training of kernel machines. 
        
        \item By exploiting the connection between tensor product and fast convolution of Count Sketches, our approximation algorithm works in time O(n(d+D log D)) for n training samples in d-dimensional space and D random features.
    \end{itemize}
    

\end{frame}


\begin{frame}
    \frametitle{Objetivos semana siguiente}

    - Disminuir coste computacional SVM (Kernel ridge, GP)
    - Deep Redes neuronales 

    Ideas simplificación: 
    -  RF
    - Nyström  
----------------
Contar todas las cosas y hacer experimentos. 


\end{frame}