\section{Network Training}

\begin{frame}
  \frametitle{The error function}


\begin{equation}
  t = y(x,w) + \epsilon 
\end{equation}
where $\epsilon \sim \mathcal{N}(0, \beta^{-1})$ 
and $\beta \in \R$ is the precision ($\beta^{-1} = \sigma^2$). 

Thus we can write
% la probabilidad t a partir de x es 
%  igual a la probabilidad de que t lo sea a partir de la normal
% abuso de notación N
% N es la función de probabilidad de una normal 
% de media y y de varianza la inversa de la precisión 
\begin{equation}
  p(t | x, w, \beta) = \mathcal{N}(t | y(x,w), \beta^{-1}). 
\end{equation}

\end{frame}


\begin{frame}
  For a single real-valued variable x, the Gaussian distribution is defined by

  \begin{equation}
    \mathcal{N}(x | \mu, \sigma^2) 
    = 
    \frac{1}{\sqrt{2 \pi \sigma^2}}
    \exp 
    \left\{
      - \frac{1}{2 \sigma^2} (x - \mu)^2
    \right\}. 
  \end{equation}

  We have to minimize the following error function
  \begin{equation}
    - \log p(t|X,w, \beta) 
    =
    \frac{n \beta}{2} 
    \sum_{i = 1}^n
    \left\{
      y(x_i, w)
      -
      t_i
    \right\}^2
    - 
    % first fraction 
    \frac{n}{2} \log \beta 
    - \frac{n}{2} \log(2 \pi) 
  \end{equation}
  which can be used to learn the parameters 
  $w$ and $\beta$. 
\end{frame}


\begin{frame}
  \frametitle{The error function}

  Let take the error function as the 
  \begin{equation}
    E(w)
    = 
    \frac{1}{2} 
    \sum_{i = 1}^n
    \left\{
      y(x_i, w)
      -
      t_i
    \right\}^2
  \end{equation}
  where we have discarded additive and multiplicative constant.

  Some considerations: 
  \begin{itemize}
    \item The value of $w$ found by minimizing $E(w)$ will be denoted as $w_{ML}$ (maximum likelihood solution).
    \item The nonlinearity of the network function $y(x_n, w)$ causes the error $E(w)$ to be nonconvex.
    \item So in practice $W_{ML}$ would be a local minimum. 
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{About the precision}

  Having found $w_{ML}$ the value of $\beta$ can be found by minimizing the negative log likelihood to give 

  \begin{equation}
    \frac{1}{\beta_{ML}}
    = 
    \frac{1}{N}
    \frac{1}{2} 
    \sum_{i = 1}^N
    \left\{
      y(x_i, w_{ML})
      -
      t_i
    \right\}^2
  \end{equation}

\end{frame}

\begin{frame}
  \frametitle{Multiple target variable}

  \begin{equation}
    p(t | x, w)
    = 
    \mathcal{N}(t | y(x,w), \beta^{-1} I). 
  \end{equation}

  The noise precision is the given by 
  \begin{equation}
    \frac{1}{\beta_{ML}}
    = 
    \frac{1}{N K}
    \sum_{i = 1}^N
    \|
    y(x_i, w_{ML}) - t_i
    \|^2
  \end{equation}
\end{frame}

\begin{frame}
  \frametitle{Minimizing regression case}
\begin{equation}
  \frac{\partial E}{\partial a_k}
  = y_k  - t_k
\end{equation}
\end{frame}

\begin{frame}
  \frametitle{Binary classification problem}
We have a single target $t$ such that $t = 1$
dentotes class $C_1$ and $t=0$ denotes class
$C_2$. 

As we see last week we consider a single output whose
activation function is a logistic sigmoid: 

\begin{equation}
  y = 
  \frac{1}{1 + exp(-a)}
\end{equation}
so that 
\begin{equation}
  0 
  \leq 
  y(x,w)
  \leq
  1.
\end{equation}
We can interpret $y(x,w)$ as the condicional probability 
$p(C_1 |x)$.
\end{frame}

\begin{frame}
  \frametitle{The cross entropy error function}
  The conditional distribution of targets given 
  inputs is the Bernoulli distribution of the form

  \begin{equation}
    p(t|x,w)
    = 
    y(x,w)^t
    \left\{
      1 - y(x,w)
    \right\}^{1-t}.
  \end{equation}

  If we consider a training set of independent observation, 
  then the error function which is given by the negative log 
  likelihood, is then a cross entropy error function of the form 

  \begin{equation}
    E(w)
    =
    - 
    \sum_{n = 1}^N
    \left\{ 
      t_n \log y_n
      +
      (1-t_n) \log(1 - y_n)
    \right\}
  \end{equation}
Using cross entropy error function instead of the sum of squares 
for classification problem leads to faster training as well as improved generalization.
\end{frame}


\begin{frame}
  \frametitle{K binary classification}

  \begin{equation}
    p(t|x,w)
    = 
    \prod_{k=1}^K
    y(x,w)^t
    \left\{
      1 - y(x,w)
    \right\}^{1-t}.
  \end{equation}

  \begin{equation}
    E(w)
    =
    - 
    \sum_{n = 1}^N
    \sum_{k = 1}^K
    \left\{ 
      t_{n k} \log y_{n k}
      +
      (1-t_{n k}) \log(1 - y_{n k})
    \right\}
  \end{equation}

\end{frame}

\begin{frame}
  \frametitle{1 of K coding scheme}
The network outputs are interpreted as
\begin{equation}
  y_k(x,w)
  = 
  p(t_k = 1 | x),
\end{equation}
leading to the following error function
  \begin{equation}
    E(w)
    = 
    - \sum_{n=1}^N
    \sum_{k=1}^K
    t_{k n}
    \log y_k(x_n, w).
  \end{equation}
\end{frame}

\section{Universal approximator}
\begin{frame}
  \frametitle{Universal approximator}

  This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.
  \cite{Multilayerfeedforwardnetworksareuniversalapproximators}

\end{frame}
\section{Backpropagation}
\begin{frame}
  \frametitle{Backpropagation}
\begin{equation}\label{eq:descenso-gradiente}
    h_{t+1}  = h_t - \eta \nabla E(h_t).
\end{equation} 
Properties 
\begin{itemize}
  \item Local optimization. 
  \item Fix the number of neurons first. 
  \item $\eta$ is a hyperparameter.  
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Our model}

  \begin{equation}\label{eq:red-neuronal-que-aprender}
    h_k(x) = 
    \sum_{j=1}^n \beta_{j k}
    \sigma
    \left(  
        \alpha_{0 j} +
        \sum_{i=1}^d \alpha_{i j}x_i
    \right)
\end{equation}
  
\end{frame}

\begin{frame}
  \frametitle{Derivative with respect to $\beta_{v w}$ st $v \in \{1, \ldots, n\}$ and $w \in \{1, \ldots, s\}$}
{\tiny
    \begin{align} \label{eq:parcial_beta}
        \frac{\partial E(h)}{\partial \beta_{v w}} 
        = &
        \frac{\partial}{\partial \beta_{v w}}
        \left[
            \frac{1}{2}
            \sum_{(x,y) \in \mathcal{D}}
            \sum_{k = 1}^s 
            \left(h_k(x) - y_k \right)^2
        \right]
        \\ % primer paso regla de la cadena
        = &
        \frac{1}{2}
        \sum_{(x,y) \in \mathcal{D}}
        \sum_{k = 1}^s 
        2 \left(h_k(x) - y_k \right)
        \frac{\partial h_k(x)}{\partial \beta_{v w}} 
        \\ 
        = & % desarrollamos h
        \sum_{(x,y) \in \mathcal{D}}
        \sum_{k = 1}^s 
        \left(h_k(x) - y_k \right)
        \frac{\partial}{\partial \beta_{v w}} 
        \left[
            \sum_{j = 1}^n 
                \beta_{j k}
                \sigma
                \left(  
                    \alpha_{0 j} +
                    \sum_{i=1}^d \alpha_{i j}x_i
                \right)
        \right] 
        \\ 
        = & % parcial de la suma es suma de parciales 
        \sum_{(x,y) \in \mathcal{D}}
        \sum_{k = 1}^s 
        \left(h_k(x) - y_k \right)
        \left(
            \sum_{j = 1}^n 
            \frac{\partial}{\partial \beta_{v w}} 
            \left[
                \beta_{j k}
                \sigma
                \left(  
                    \alpha_{0 j} +
                    \sum_{i=1}^d \alpha_{i j}x_i
                \right)
            \right]
        \right) 
        \\ 
        = & % Expresión a partir de la función caraterísticas
        \sum_{(x,y) \in \mathcal{D}}
        \sum_{k = 1}^s 
        \left(h_k(x) - y_k \right)
        \left(
            \sum_{j = 1}^n 
                \chi_{[j = v \wedge k = w]}
                \sigma
                \left(  
                    \alpha_{0 j} +
                    \sum_{j=1}^d \alpha_{i j}x_i
                \right)
        \right)
        \\ 
        = & % Expresión final quitando términos nulos 
        \sum_{(x,y) \in \mathcal{D}}
        \left(h_w(x) - y_w \right)
        \left(
            \sigma
            \left(  
                \alpha_{0 v} +
                \sum_{i=1}^d \alpha_{i v}x_i
            \right)
        \right).
    \end{align}
  }
\end{frame}

\begin{frame}
  \frametitle{Derivative with respect to $\alpha_{0 v}$ st $v \in \{1, \ldots, n\}$}
  {\tiny 
  \begin{align} \label{eq:parcial_alpha_cero}
        \frac{\partial E(h)}{\partial \alpha_{0 v}} 
        = &
        \frac{\partial}{\partial \alpha_{0 v}}
        \left[
            \frac{1}{2}
            \sum_{(x,y) \in \mathcal{D}}
            \sum_{k = 1}^s 
            \left(h_k(x) - y\right)^2
        \right]
        \\ % primer paso regla de la cadena
        = &
        \frac{1}{2}
        \sum_{(x,y) \in \mathcal{D}}
        \sum_{k = 1}^s 
        2 \left(h_k(x) - y_k \right)
        \frac{\partial h_k(x)}{\partial \alpha_{0 v}} 
        \\ 
        = & % desarrollamos h
        \sum_{(x,y) \in \mathcal{D}}
        \sum_{k = 1}^s 
        \left(h_k(x) - y_k \right)
        \frac{\partial}{\partial \alpha_{0 v}} 
        \left[
            \sum_{j = 1}^n 
                \beta_{j k}
                \sigma
                \left(  
                    \alpha_{0 j} +
                    \sum_{i=1}^d \alpha_{i j}x_i
                \right)
        \right] 
        \\ 
        = & % parcial de la suma es suma de parciales 
        \sum_{(x,y) \in \mathcal{D}}
        \sum_{k = 1}^s 
        \left(h_k(x) - y_k \right)
        \left(
            \sum_{j = 1}^n 
            \beta_{j k}
            \frac{\partial}{\partial \alpha_{0 v}} 
            \left[
                \sigma
                \left(  
                    \alpha_{0 j} +
                    \sum_{i=1}^d \alpha_{i j}x_i
                \right)
            \right]
        \right) 
        \\ 
        = & %regla de la cadena 
        \sum_{(x,y) \in \mathcal{D}}
        \sum_{k = 1}^s 
        \left(h_k(x) - y_k \right)
        \left(
            \sum_{j = 1}^n 
            \beta_{j k}
            \sigma '
            \left(  
                \alpha_{0 j} +
                \sum_{i=1}^d \alpha_{i j}x_i
            \right)
            \frac{\partial}{\partial \alpha_{0 v}}    
            \left[
                \alpha_{0 j} +
                \sum_{i=1}^d \alpha_{i j}x_i
            \right]
        \right) 
        \\ 
        = & % función característica
        \sum_{(x,y) \in \mathcal{D}}
        \sum_{k = 1}^s 
        \left(h_k(x) - y_k \right)
        \left(
            \sum_{j = 1}^n 
            \beta_{j k}
            \sigma '
            \left(  
                \alpha_{0 j} +
                \sum_{i=1}^d \alpha_{i j}x_i
            \right)   
            \chi_{[j = v]}
        \right) 
        \\ 
        = & % Expresión final quitando términos nulos 
        \sum_{(x,y) \in \mathcal{D}}
        \sum_{k = 1}^s 
        \left(h_k(x) - y_k \right)
        \left(
            \beta_{v k}
            \sigma '
            \left(  
                \alpha_{0 v} +
                \sum_{i=1}^d \alpha_{i v}x_i
            \right)   
        \right). 
    \end{align}
  }
\end{frame}

\begin{frame}
  \frametitle{Derivative with respect $\alpha_{u v}$ st $u \in \{1, \ldots, d\}$ and $v \in \{1, \ldots, n\}$}

  {\tiny
  \begin{align} \label{eq:parcial_alpha_i}
    \frac{\partial E(h)}{\partial \alpha_{u v}} 
    =&
    \frac{\partial}{\partial \alpha_{u v}}
    \left[
        \frac{1}{2}
        \sum_{(x,y) \in \mathcal{D}}
        \sum_{k = 1}^s 
        \left(h_k(x) - y_k \right)^2
    \right]
    \\ % primer paso regla de la cadena
    = &
    \frac{1}{2}
    \sum_{(x,y) \in \mathcal{D}}
    \sum_{k = 1}^s 
    2 \left(h_k(x) - y_k \right)
    \frac{\partial h_k(x)}{\partial \alpha_{u v}} 
    \\ 
    = & % desarrollamos h
    \sum_{(x,y) \in \mathcal{D}}
    \sum_{k = 1}^s 
    \left(h_k(x) - y_k \right)
    \frac{\partial}{\partial \alpha_{u v}} 
    \left[
        \sum_{j = 1}^n 
            \beta_{j k}
            \sigma
            \left(  
                \alpha_{0 j} +
                \sum_{i=1}^d \alpha_{i j}x_i
            \right)
    \right] 
    \\ 
    = & % parcial de la suma es suma de parciales 
    \sum_{(x,y) \in \mathcal{D}}
    \sum_{k = 1}^s 
    \left(h_k(x) - y_k \right)
    \left(
        \sum_{j = 1}^n 
        \beta_{j k}
        \frac{\partial}{\partial \alpha_{u v}} 
        \left[
            \sigma
            \left(  
                \alpha_{0 j} +
                \sum_{i=1}^d \alpha_{i j}x_i
            \right)
        \right]
    \right) 
    \\ 
    = & %regla de la cadena 
    \sum_{(x,y) \in \mathcal{D}}
    \sum_{k = 1}^s 
    \left(h_k(x) - y_k \right)
    \left(
        \sum_{j = 1}^n 
        \beta_{j k}
        \sigma '
        \left(  
            \alpha_{0 j} +
            \sum_{i=1}^d \alpha_{i j}x_i
        \right)
        \frac{\partial}{\partial \alpha_{u v}}    
        \left[
            \alpha_{0 j} +
            \sum_{i=1}^d \alpha_{i j}x_i
        \right]
    \right) 
    \\ 
    = & % función característica
    \sum_{(x,y) \in \mathcal{D}}
    \sum_{k = 1}^s 
    \left(h_k(x) - y_k \right)
    \left(
        \sum_{j = 1}^n 
        \beta_{j k}
        \sigma '
        \left(  
            \alpha_{0 j} +
            \sum_{i=1}^d \alpha_{i j}x_i
        \right)   
        \chi_{[i = u\wedge j = v]}x_i
    \right) 
    \\ 
    = & % Expresión final quitando términos nulos 
    \sum_{(x,y) \in \mathcal{D}}
    \sum_{k = 1}^s 
    \left(h_k(x) - y_k \right)
    \left(
        \beta_{v k}
        \sigma '
        \left(  
            \alpha_{0 v} +
            \sum_{i=1}^d \alpha_{i v}x_i
        \right)x_u   
    \right).
\end{align}
  }
\end{frame}


\begin{frame}
  \frametitle{Summary}

  {\tiny
    \begin{align} 
        \frac{\partial E(h)}{\partial \beta_{v w}} 
        = & % Expresión final quitando términos nulos 
        \sum_{(x,y) \in \mathcal{D}}
        \left(h_w(x) - y_w \right)
        \left(
            \sigma
            \left(  
                \alpha_{0 v} +
                \sum_{i=1}^d \alpha_{i v}x_i
            \right)
        \right).
    \end{align}

    
    \begin{align} 
        \frac{\partial E(h)}{\partial \alpha_{0 v}} 
        =  % Expresión final quitando términos nulos 
        \sum_{(x,y) \in \mathcal{D}}
        \sum_{k = 1}^s 
        \left(h_k(x) - y_k \right)
        \left(
            \beta_{v k}
            \sigma '
            \left(  
                \alpha_{0 v} +
                \sum_{i=1}^d \alpha_{i v}x_i
            \right)   
        \right). 
    \end{align}

    
    \begin{align} 
        \frac{\partial E(h)}{\partial \alpha_{u v}} 
        =
        \sum_{(x,y) \in \mathcal{D}}
        \sum_{k = 1}^s 
        \left(h_k(x) - y_k\right)
        \left(
            \beta_{v k}
            \sigma '
            \left(  
                \alpha_{0 v} +
                \sum_{i=1}^d \alpha_{i v}x_i
            \right)x_u   
        \right).
    \end{align}
  }

\end{frame}