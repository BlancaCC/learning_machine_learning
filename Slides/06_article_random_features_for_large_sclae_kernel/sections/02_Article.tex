\section*{Understanding the difficulty of training deep feedforward neural networks}

\begin{frame}
    \frametitle{Main points of the article}

    \begin{itemize}
        \item 2006 deep neural network show their power with new initialization an training mechanisms.
        \item Logistic sigmoid activation is unsuited for deep networiks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. 
    
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Questions?}
    Why are deeper networks better than wider?

    Intuitively approach piecewise polynomial functions.
\end{frame}