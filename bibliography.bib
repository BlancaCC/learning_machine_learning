/**
Main bibliography
*/
@book{BishopPatternRecognition, 
    author = {Bishop, Christopher M.}, 
    title = {Pattern Recognition and Machine Learning (Information Science and Statistics)}, 
    year = {2006}, 
    isbn = {0387310738}, 
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg} 
}
/**
Main bibliography.
More rigorous at statistic than  BishopPatternRecognition
*/
@book{HastieStatisticalLearing,
  added-at = {2008-05-16T16:17:42.000+0200},
  address = {New York, NY, USA},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  interhash = {d585aea274f2b9b228fc1629bc273644},
  intrahash = {f58afc5c9793fcc8ad8389824e57984c},
  keywords = {ml statistics},
  publisher = {Springer New York Inc.},
  series = {Springer Series in Statistics},
  timestamp = {2008-05-16T16:17:43.000+0200},
  title = {The Elements of Statistical Learning},
  year = 2001
}

//--------------------------------------------
//              Approximation theory
//--------------------------------------------

/**
  Approximation theory from basic to more complex vision. 
  Some relevant content: 
  - Interpolation (Tensor-product, Newton, Lagrange).
  - Approximation by: definite functions, convolution
  - Kernels
  - Ridge function
  - Artificial networks as a approximators
  - Splines
  - Wavelets
*/
@book{ACourseInApproximationTheory,
  author         = {Ward Cheney and Will Light},
  publisher      = {American Mathematical Society},
  title          = {A Course in Approximation Theory},
  year           = {2009},
  isbn           = {9780821847985},
  series         = {Series: Graduate Studies in Mathematics 101}
}


@online{LebesgueConstant,
  title = {{Lebesgue constant}},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Lebesgue_constant},
  urldate = {2022-10-15}
}
/**
  Runge's phenomenon
  Error in approximation 
*/
@online{RungePhenomenon,
  title = {{Runge's phenomenon}},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Runge%27s_phenomenon},
  urldate = {2022-10-15}
}
@online{Splines,
  title = {{Spline (mathematics)}},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Spline_(mathematics)},
  urldate = {2022-10-15}
}

@online{Wavelet,
  title = {Wavelet},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Wavelet},
  urldate = {2022-10-15}
}

//----------------------------------------
//             Arquitecture
//------------------------------------------
@inproceedings{AUniversalLawofRobustness,
title={A Universal Law of Robustness via Isoperimetry},
author={Sebastien Bubeck and Mark Sellke},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=z71OSKqTFh7}
}


// -----------------------------------------------
// Convergence theorem
// -----------------------------------------------
@article{Multilayerfeedforwardnetworksareuniversalapproximators,
    title = {Multilayer feedforward networks are universal approximators},
    journal = {Neural Networks},
    volume = {2},
    number = {5},
    pages = {359-366},
    year = {1989},
    issn = {0893-6080},
    doi = {https://doi.org/10.1016/0893-6080(89)90020-8},
    url = {https://www.sciencedirect.com/science/article/pii/0893608089900208},
    author = {Kurt Hornik and Maxwell Stinchcombe and Halbert White},
    keywords = {Feedforward networks, Universal approximation, Mapping networks, Network representation capability, Stone-Weierstrass Theorem, Squashing functions, Sigma-Pi networks, Back-propagation networks},
    abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.}
}



@InProceedings{Understanding_the_difficulty_of_training_deep_feedforward_neural_networks,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}


//-----------------------------------------------
//          Interesting people
//-----------------------------------------------

/** Richard S. Sutton
Sutton is considered one of the founders of modern computational reinforcement learning
*/ 
@online{WikipediaPageRicharSutton,
  title = {Richard S. Sutton, wikipedia information},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Richard_S._Sutton},
  urldate = {2022-11-10}
}
@online{RichardSuttonWebpage,
  title = {Richard S. Sutton, personal webpage},
  year = 2022,
  url = {http://incompleteideas.net},
  urldate = {2022-11-10}
}



/**-----------------------------------------------------------------
NEURAL NETWORKS
-----------------------------------------------------------------
*/
//------- Activation function ------ 

@article{NonpolynomialActivationFunction,
title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
journal = {Neural Networks},
volume = {6},
number = {6},
pages = {861-867},
year = {1993},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(05)80131-5},
url = {https://www.sciencedirect.com/science/article/pii/S0893608005801315},
author = {Moshe Leshno and Vladimir Ya. Lin and Allan Pinkus and Shimon Schocken},
keywords = {Multilayer feedforward networks, Activation functions, Role of threshold, Universal approximation capabilities, (μ) approximation},
abstract = {Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold.}
}

// ------ Diferent architectures ------ 

@article{FundamentalsOfRecurrentNerualNetwork,
  author    = {Alex Sherstinsky},
  title     = {Fundamentals of Recurrent Neural Network {(RNN)} and Long Short-Term
               Memory {(LSTM)} Network},
  journal   = {CoRR},
  volume    = {abs/1808.03314},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.03314},
  eprinttype = {arXiv},
  eprint    = {1808.03314},
  timestamp = {Sun, 02 Sep 2018 15:01:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-03314.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

//--------------------------------------------
// Opinion articles 
//--------------------------------------------

/**
And the human-knowledge approach tends to complicate methods in ways that make them less suited to taking advantage of general methods leveraging computation. 

we should stop trying to find simple ways to think about the contents of minds, such as simple ways to think about space,
 objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, outside world. They are not what should be built in,
  as their complexity is endless; instead we should build in 
  only the meta-methods that can find and capture this 
  arbitrary complexity. 
*/
@online{TheBitterLesson,
  title = {The Bitter Lesson},
  year = 2019,
  url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
  urldate = {2022-11-10}
}


/**------------------------------------
Optimization 
-------------------------------------*/

@misc{SmallBatchSize,
  doi = {10.48550/ARXIV.1804.07612},
  
  url = {https://arxiv.org/abs/1804.07612},
  
  author = {Masters, Dominic and Luschi, Carlo},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Revisiting Small Batch Training for Deep Neural Networks},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{Adam,
  doi = {10.48550/ARXIV.1412.6980},
  
  url = {https://arxiv.org/abs/1412.6980},
  
  author = {Kingma, Diederik P. and Ba, Jimmy},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adam: A Method for Stochastic Optimization},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@online{ConjugateGradientMethod,
  title = {Conjugate gradient methods},
  year = 2023,
  url = {https://en.wikipedia.org/wiki/Conjugate_gradient_method},
  urldate = {2023-1-27},
  author = {wikipedia: Conjugate gradient methods}
}


@article{OptimizingNeuralNetworksintheEquivalentClassSpace,
author = {Meng, Qi and Chen, Wei and Zheng, Shuxin and Ye, Qiwei and Liu, Tie-Yan},
year = {2018},
month = {02},
pages = {},
title = {Optimizing Neural Networks in the Equivalent Class Space}
}

@online{QuadraticPrograming,
  title = {Quadratic Programming},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Quadratic_programming},
  urldate = {2022-11-10}
}

@online{Quasi-NewtonMethod,
  title = {Quasi Newton method},
  year = 2023,
  url = {https://en.wikipedia.org/wiki/Quasi-Newton_method},
  urldate = {2023-1-27},
  author = {wikipedia: Quasi Newton method}
}

@online{Limited-memoryBFGS,
  title = {Limited-memory BFGS},
  year = 2023,
  url = {https://en.wikipedia.org/wiki/Limited-memory_BFGS},
  urldate = {2023-1-27},
  author = {wikipedia: Limited-memory BFGS}
}

@online{WhyMomentumReallyWorks,
  title = {Why Momentum Really Works},
  year = 2023,
  url = {https://distill.pub/2017/momentum/},
  urldate = {2023-1-27},
  author = {Why Momentum Really Works}
}

//----------------------------------------
// Quantile Regression
//----------------------------------------

// Quantil regression Wikipedia
@online{QuantilRegression,
  title = {Quantile Regression},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Quantile_regression},
  urldate = {2022-11-10}
}

// Blog 
@online{QuantilRegressionInMachineLearning,
  title = {Quantile Regression in machine learning},
  year = 2018,
  url = {https://towardsdatascience.com/quantile-regression-from-linear-models-to-trees-to-deep-learning-af3738b527c3},
  urldate = {2022-11-10}
}

// Article
@misc{QuantileRefressionForSpatioTemporalProblem,
  doi = {10.48550/ARXIV.1808.08798},
  url = {https://arxiv.org/abs/1808.08798},
  author = {Rodrigues, Filipe and Pereira, Francisco C.},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Beyond expectation: Deep joint mean and quantile regression for spatio-temporal problems},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
//--------------------------------------
// Regularización 
//--------------------------------------

//Qué es una matriz definida positiva
@online{MatrizDefinidaPositiva,
  title = {Matriz definida positiva},
  year = 2022,
  url = {https://es.wikipedia.org/wiki/Matriz_definida_positiva},
  urldate = {2022-11-10}
}

// Qué es un producto interno o escalar 
@online{ProductoEscalar,
  title = {Producto escalar},
  year = 2022,
  url = {Producto escalar},
  urldate = {2022-11-10}
}

//--------------------------------------
// Statistic 
//--------------------------------------

/**
Information about linear model and link function
*/
@online{link_function,
  title = {Matriz definida positiva},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Generalized_linear_model},
  urldate = {2022-11-19}
}


//------------------------
// Transfer learning 
//-------------------------

@article{SurveyonTransferLearning,
  author    = {Fuzhen Zhuang and
               Zhiyuan Qi and
               Keyu Duan and
               Dongbo Xi and
               Yongchun Zhu and
               Hengshu Zhu and
               Hui Xiong and
               Qing He},
  title     = {A Comprehensive Survey on Transfer Learning},
  journal   = {CoRR},
  volume    = {abs/1911.02685},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.02685},
  eprinttype = {arXiv},
  eprint    = {1911.02685},
  timestamp = {Sat, 29 Aug 2020 18:19:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02685.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{aji-etal-2020-neural,
    title = "In Neural Machine Translation, What Does Transfer Learning Transfer?",
    author = "Aji, Alham Fikri  and
      Bogoychev, Nikolay  and
      Heafield, Kenneth  and
      Sennrich, Rico",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.688",
    doi = "10.18653/v1/2020.acl-main.688",
    pages = "7701--7710",
    abstract = "Transfer learning improves quality for low-resource machine translation, but it is unclear what exactly it transfers. We perform several ablation studies that limit information transfer, then measure the quality impact across three language pairs to gain a black-box understanding of transfer learning. Word embeddings play an important role in transfer learning, particularly if they are properly aligned. Although transfer learning can be performed without embeddings, results are sub-optimal. In contrast, transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warm-up phase when training transformer models in high resource language pairs.",
}