/**
Main bibliography
*/
@book{BishopPatternRecognition, 
    author = {Bishop, Christopher M.}, 
    title = {Pattern Recognition and Machine Learning (Information Science and Statistics)}, 
    year = {2006}, 
    isbn = {0387310738}, 
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg} 
}
/**
Main bibliography.
More rigorous at statistic than  BishopPatternRecognition
*/
@book{HastieStatisticalLearing,
  added-at = {2008-05-16T16:17:42.000+0200},
  address = {New York, NY, USA},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  interhash = {d585aea274f2b9b228fc1629bc273644},
  intrahash = {f58afc5c9793fcc8ad8389824e57984c},
  keywords = {ml statistics},
  publisher = {Springer New York Inc.},
  series = {Springer Series in Statistics},
  timestamp = {2008-05-16T16:17:43.000+0200},
  title = {The Elements of Statistical Learning},
  year = 2001
}

//--------------------------------------------
//              Approximation theory
//--------------------------------------------

/**
  Approximation theory from basic to more complex vision. 
  Some relevant content: 
  - Interpolation (Tensor-product, Newton, Lagrange).
  - Approximation by: definite functions, convolution
  - Kernels
  - Ridge function
  - Artificial networks as a approximators
  - Splines
  - Wavelets
*/
@book{ACourseInApproximationTheory,
  author         = {Ward Cheney and Will Light},
  publisher      = {American Mathematical Society},
  title          = {A Course in Approximation Theory},
  year           = {2009},
  isbn           = {9780821847985},
  series         = {Series: Graduate Studies in Mathematics 101}
}


@online{LebesgueConstant,
  title = {{Lebesgue constant}},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Lebesgue_constant},
  urldate = {2022-10-15}
}
/**
  Runge's phenomenon
  Error in approximation 
*/
@online{RungePhenomenon,
  title = {{Runge's phenomenon}},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Runge%27s_phenomenon},
  urldate = {2022-10-15}
}
@online{Splines,
  title = {{Spline (mathematics)}},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Spline_(mathematics)},
  urldate = {2022-10-15}
}

@online{Wavelet,
  title = {Wavelet},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Wavelet},
  urldate = {2022-10-15}
}

//----------------------------------------
//             Arquitecture
//------------------------------------------
@inproceedings{AUniversalLawofRobustness,
title={A Universal Law of Robustness via Isoperimetry},
author={Sebastien Bubeck and Mark Sellke},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=z71OSKqTFh7}
}

//-----------------------------------------------
//          Interesting people
//-----------------------------------------------

/** Richard S. Sutton
Sutton is considered one of the founders of modern computational reinforcement learning
*/ 
@online{WikipediaPageRicharSutton,
  title = {Richard S. Sutton, wikipedia information},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Richard_S._Sutton},
  urldate = {2022-11-10}
}
@online{RichardSuttonWebpage,
  title = {Richard S. Sutton, personal webpage},
  year = 2022,
  url = {http://incompleteideas.net},
  urldate = {2022-11-10}
}



/**-----------------------------------------------------------------
NEURAL NETWORKS
-----------------------------------------------------------------
*/
//------- Activation function ------ 

@article{NonpolynomialActivationFunction,
title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
journal = {Neural Networks},
volume = {6},
number = {6},
pages = {861-867},
year = {1993},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(05)80131-5},
url = {https://www.sciencedirect.com/science/article/pii/S0893608005801315},
author = {Moshe Leshno and Vladimir Ya. Lin and Allan Pinkus and Shimon Schocken},
keywords = {Multilayer feedforward networks, Activation functions, Role of threshold, Universal approximation capabilities, (μ) approximation},
abstract = {Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold.}
}

// ------ Diferent architectures ------ 

@article{FundamentalsOfRecurrentNerualNetwork,
  author    = {Alex Sherstinsky},
  title     = {Fundamentals of Recurrent Neural Network {(RNN)} and Long Short-Term
               Memory {(LSTM)} Network},
  journal   = {CoRR},
  volume    = {abs/1808.03314},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.03314},
  eprinttype = {arXiv},
  eprint    = {1808.03314},
  timestamp = {Sun, 02 Sep 2018 15:01:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-03314.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

//--------------------------------------------
// Opinion articles 
//--------------------------------------------

/**
And the human-knowledge approach tends to complicate methods in ways that make them less suited to taking advantage of general methods leveraging computation. 

we should stop trying to find simple ways to think about the contents of minds, such as simple ways to think about space,
 objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, outside world. They are not what should be built in,
  as their complexity is endless; instead we should build in 
  only the meta-methods that can find and capture this 
  arbitrary complexity. 
*/
@online{TheBitterLesson,
  title = {The Bitter Lesson},
  year = 2019,
  url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
  urldate = {2022-11-10}
}


/**------------------------------------
Optimization 
-------------------------------------*/

@article{OptimizingNeuralNetworksintheEquivalentClassSpace,
author = {Meng, Qi and Chen, Wei and Zheng, Shuxin and Ye, Qiwei and Liu, Tie-Yan},
year = {2018},
month = {02},
pages = {},
title = {Optimizing Neural Networks in the Equivalent Class Space}
}

@online{QuadraticPrograming,
  title = {Quadratic Programming},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Quadratic_programming},
  urldate = {2022-11-10}
}

//----------------------------------------
// Quantile Regression
//----------------------------------------

// Quantil regression Wikipedia
@online{QuantilRegression,
  title = {Quantile Regression},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Quantile_regression},
  urldate = {2022-11-10}
}

// Blog 
@online{QuantilRegressionInMachineLearning,
  title = {Quantile Regression in machine learning},
  year = 2018,
  url = {https://towardsdatascience.com/quantile-regression-from-linear-models-to-trees-to-deep-learning-af3738b527c3},
  urldate = {2022-11-10}
}

// Article
@misc{QuantileRefressionForSpatioTemporalProblem,
  doi = {10.48550/ARXIV.1808.08798},
  url = {https://arxiv.org/abs/1808.08798},
  author = {Rodrigues, Filipe and Pereira, Francisco C.},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Beyond expectation: Deep joint mean and quantile regression for spatio-temporal problems},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
//--------------------------------------
// Regularización 
//--------------------------------------

//Qué es una matriz definida positiva
@online{MatrizDefinidaPositiva,
  title = {Matriz definida positiva},
  year = 2022,
  url = {https://es.wikipedia.org/wiki/Matriz_definida_positiva},
  urldate = {2022-11-10}
}

// Qué es un producto interno o escalar 
@online{ProductoEscalar,
  title = {Producto escalar},
  year = 2022,
  url = {Producto escalar},
  urldate = {2022-11-10}
}

//--------------------------------------
// Statistic 
//--------------------------------------

/**
Information about linear model and link function
*/
@online{link_function,
  title = {Matriz definida positiva},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Generalized_linear_model},
  urldate = {2022-11-19}
}


//------------------------
// Transfer learning 
//-------------------------

@article{SurveyonTransferLearning,
  author    = {Fuzhen Zhuang and
               Zhiyuan Qi and
               Keyu Duan and
               Dongbo Xi and
               Yongchun Zhu and
               Hengshu Zhu and
               Hui Xiong and
               Qing He},
  title     = {A Comprehensive Survey on Transfer Learning},
  journal   = {CoRR},
  volume    = {abs/1911.02685},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.02685},
  eprinttype = {arXiv},
  eprint    = {1911.02685},
  timestamp = {Sat, 29 Aug 2020 18:19:14 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-02685.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{aji-etal-2020-neural,
    title = "In Neural Machine Translation, What Does Transfer Learning Transfer?",
    author = "Aji, Alham Fikri  and
      Bogoychev, Nikolay  and
      Heafield, Kenneth  and
      Sennrich, Rico",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.688",
    doi = "10.18653/v1/2020.acl-main.688",
    pages = "7701--7710",
    abstract = "Transfer learning improves quality for low-resource machine translation, but it is unclear what exactly it transfers. We perform several ablation studies that limit information transfer, then measure the quality impact across three language pairs to gain a black-box understanding of transfer learning. Word embeddings play an important role in transfer learning, particularly if they are properly aligned. Although transfer learning can be performed without embeddings, results are sub-optimal. In contrast, transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warm-up phase when training transformer models in high resource language pairs.",
}