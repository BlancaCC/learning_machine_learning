/**
Main bibliography
*/
@book{BishopPatternRecognition, 
    author = {Bishop, Christopher M.}, 
    title = {Pattern Recognition and Machine Learning (Information Science and Statistics)}, 
    year = {2006}, 
    isbn = {0387310738}, 
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg} 
}
/**
Main bibliography.
More rigorous at statistic than  BishopPatternRecognition
*/
@book{HastieStatisticalLearing,
  added-at = {2008-05-16T16:17:42.000+0200},
  address = {New York, NY, USA},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  interhash = {d585aea274f2b9b228fc1629bc273644},
  intrahash = {f58afc5c9793fcc8ad8389824e57984c},
  keywords = {ml statistics},
  publisher = {Springer New York Inc.},
  series = {Springer Series in Statistics},
  timestamp = {2008-05-16T16:17:43.000+0200},
  title = {The Elements of Statistical Learning},
  year = 2001
}

//--------------------------------------------
//              Approximation theory
//--------------------------------------------

/**
  Approximation theory from basic to more complex vision. 
  Some relevant content: 
  - Interpolation (Tensor-product, Newton, Lagrange).
  - Approximation by: definite functions, convolution
  - Kernels
  - Ridge function
  - Artificial networks as a approximators
  - Splines
  - Wavelets
*/
@book{ACourseInApproximationTheory,
  author         = {Ward Cheney and Will Light},
  publisher      = {American Mathematical Society},
  title          = {A Course in Approximation Theory},
  year           = {2009},
  isbn           = {9780821847985},
  series         = {Series: Graduate Studies in Mathematics 101}
}


@online{LebesgueConstant,
  title = {{Lebesgue constant}},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Lebesgue_constant},
  urldate = {2022-10-15}
}
/**
  Runge's phenomenon
  Error in approximation 
*/
@online{RungePhenomenon,
  title = {{Runge's phenomenon}},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Runge%27s_phenomenon},
  urldate = {2022-10-15}
}
@online{Splines,
  title = {{Spline (mathematics)}},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Spline_(mathematics)},
  urldate = {2022-10-15}
}

@online{Wavelet,
  title = {Wavelet},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Wavelet},
  urldate = {2022-10-15}
}

//-----------------------------------------------
//          Interesting people
//-----------------------------------------------

/** Richard S. Sutton
Sutton is considered one of the founders of modern computational reinforcement learning
*/ 
@online{WikipediaPageRicharSutton,
  title = {Richard S. Sutton, wikipedia information},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Richard_S._Sutton},
  urldate = {2022-11-10}
}
@online{RichardSuttonWebpage,
  title = {Richard S. Sutton, personal webpage},
  year = 2022,
  url = {http://incompleteideas.net},
  urldate = {2022-11-10}
}



/**-----------------------------------------------------------------
NEURAL NETWORKS
-----------------------------------------------------------------
*/
//------- Activation function ------ 

@article{NonpolynomialActivationFunction,
title = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
journal = {Neural Networks},
volume = {6},
number = {6},
pages = {861-867},
year = {1993},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(05)80131-5},
url = {https://www.sciencedirect.com/science/article/pii/S0893608005801315},
author = {Moshe Leshno and Vladimir Ya. Lin and Allan Pinkus and Shimon Schocken},
keywords = {Multilayer feedforward networks, Activation functions, Role of threshold, Universal approximation capabilities, (μ) approximation},
abstract = {Several researchers characterized the activation function under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold.}
}

//--------------------------------------------
// Opinion articles 
//--------------------------------------------

/**
And the human-knowledge approach tends to complicate methods in ways that make them less suited to taking advantage of general methods leveraging computation. 

we should stop trying to find simple ways to think about the contents of minds, such as simple ways to think about space,
 objects, multiple agents, or symmetries. All these are part of the arbitrary, intrinsically-complex, outside world. They are not what should be built in,
  as their complexity is endless; instead we should build in 
  only the meta-methods that can find and capture this 
  arbitrary complexity. 
*/
@online{TheBitterLesson,
  title = {The Bitter Lesson},
  year = 2019,
  url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
  urldate = {2022-11-10}
}


/**------------------------------------
Optimization 
-------------------------------------*/

@online{QuadraticPrograming,
  title = {Quadratic Programming},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Quadratic_programming},
  urldate = {2022-11-10}
}

//----------------------------------------
// Quantile Regression
//----------------------------------------

// Quantil regression Wikipedia
@online{QuantilRegression,
  title = {Quantile Regression},
  year = 2022,
  url = {https://en.wikipedia.org/wiki/Quantile_regression},
  urldate = {2022-11-10}
}

// Blog 
@online{QuantilRegressionInMachineLearning,
  title = {Quantile Regression in machine learning},
  year = 2018,
  url = {https://towardsdatascience.com/quantile-regression-from-linear-models-to-trees-to-deep-learning-af3738b527c3},
  urldate = {2022-11-10}
}

// Article
@misc{QuantileRefressionForSpatioTemporalProblem,
  doi = {10.48550/ARXIV.1808.08798},
  url = {https://arxiv.org/abs/1808.08798},
  author = {Rodrigues, Filipe and Pereira, Francisco C.},
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Beyond expectation: Deep joint mean and quantile regression for spatio-temporal problems},
  publisher = {arXiv},
  year = {2018},
  copyright = {arXiv.org perpetual, non-exclusive license}
}
//--------------------------------------
// Regularización 
//--------------------------------------

//Qué es una matriz definida positiva
@online{MatrizDefinidaPositiva,
  title = {Matriz definida positiva},
  year = 2022,
  url = {https://es.wikipedia.org/wiki/Matriz_definida_positiva},
  urldate = {2022-11-10}
}

// Qué es un producto interno o escalar 
@online{ProductoEscalar,
  title = {Producto escalar},
  year = 2022,
  url = {Producto escalar},
  urldate = {2022-11-10}
}
